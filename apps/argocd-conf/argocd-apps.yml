apiVersion: argoproj.io/v1alpha1
kind: AppProject
metadata:
  name: k3s
spec:
  clusterResourceWhitelist:
  - group: '*'
    kind: '*'
  description: Home automation and management
  destinations:
  - name: in-cluster
    namespace: '*'
    server: https://kubernetes.default.svc
  namespaceResourceWhitelist:
  - group: '*'
    kind: '*'
  sourceRepos:
  - git@github.com:sholdee/home-ops.git
---
apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: k3s-apps
spec:
  goTemplate: true
  goTemplateOptions: ["missingkey=error"]
  generators:
    - git:
        repoURL: 'git@github.com:sholdee/home-ops.git'
        revision: master
        directories:
          - path: 'apps/*'
  template:
    metadata:
      name: '{{.path.basename}}'
    spec:
      project: k3s
      source:
        repoURL: 'git@github.com:sholdee/home-ops.git'
        targetRevision: master
        path: '{{.path.path}}'
        directory:
          recurse: true
      destination:
        name: in-cluster
        namespace: '{{ regexReplaceAll "-conf$" .path.basename "" }}'
      syncPolicy:
        automated:
          prune: true
        syncOptions:
        - CreateNamespace=true
        - ApplyOutOfSyncOnly=true
---
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: argocd
spec:
  destination:
    name: in-cluster
    namespace: argocd
  project: k3s
  source:
    chart: argo-cd
    helm:
      values: |-
        configs:
          cm:
            resource.exclusions: |
             - apiGroups:
                 - cilium.io
               kinds:
                 - CiliumIdentity
               clusters:
                 - "*"
    repoURL: https://argoproj.github.io/argo-helm
    targetRevision: 7.1.4
  syncPolicy:
    automated:
      prune: true
    syncOptions:
    - CreateNamespace=true
    - ApplyOutOfSyncOnly=true
---
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: cert-manager
spec:
  destination:
    name: in-cluster
    namespace: cert-manager
  project: k3s
  source:
    chart: cert-manager
    helm:
      parameters:
      - name: installCRDs
        value: "true"
      - name: dns01RecursiveNameservers
        value: 1.1.1.1:53,1.0.0.1:53
      - name: dns01RecursiveNameserversOnly
        value: "true"
    repoURL: https://charts.jetstack.io
    targetRevision: v1.15.0
  syncPolicy:
    automated:
      prune: true
    syncOptions:
    - CreateNamespace=true
    - ApplyOutOfSyncOnly=true
---
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: cilium
spec:
  destination:
    name: in-cluster
    namespace: kube-system
  ignoreDifferences:
    - jsonPointers:
      - /data/ca.crt
      - /data/ca.key
      kind: Secret
      name: cilium-ca
    - jsonPointers:
      - /data/ca.crt
      kind: Secret
      name: hubble-ca-cert
    - jsonPointers:
      - /data/ca.crt
      - /data/tls.crt
      - /data/tls.key
      kind: Secret
      name: hubble-relay-client-certs
    - jsonPointers:
      - /data/ca.crt
      - /data/tls.crt
      - /data/tls.key
      kind: Secret
      name: hubble-server-certs
  project: k3s
  source:
    chart: cilium
    helm:
      values: |-
        ipam:
          operator:
            clusterPoolIPv4PodCIDRList: "10.52.0.0/16"
        ipv4NativeRoutingCIDR: "10.52.0.0/16"
        k8sServiceHost: "127.0.0.1"
        k8sServicePort: "6444"
        routingMode: "native"
        autoDirectNodeRoutes: true
        kubeProxyReplacement: true
        bpf:
          masquerade: false
          loadBalancer:
            algorithm: "maglev"
            mode: "hybrid"
        enableIPv4Masquerade: false
        bgpControlPlane:
          enabled: true
        hubble:
          enabled: true
          relay:
            enabled: true
          ui:
            enabled: true
        installNoConntrackIptablesRules: true
        operator:
          replicas: "1"
          prometheus:
            enabled: true
            serviceMonitor:
              enabled: true
          dashboards:
            enabled: true
        prometheus:
          enabled: true
          serviceMonitor:
            enabled: true
            trustCRDsExist: true
        dashboards:
          enabled: true
    repoURL: https://helm.cilium.io/
    targetRevision: 1.15.6
  syncPolicy:
    automated:
      prune: true
    syncOptions:
    - ApplyOutOfSyncOnly=true
    - RespectIgnoreDifferences=true
---
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: ingress-nginx
spec:
  destination:
    name: in-cluster
    namespace: ingress-nginx
  project: k3s
  source:
    chart: ingress-nginx
    helm:
      parameters:
      - name: controller.service.internal.loadBalancerIP
        value: 192.168.77.20
      values: |-
        controller:
          extraArgs:
            default-ssl-certificate: "ingress-nginx/wildcard-cert"
          metrics:
            enabled: true
            serviceMonitor:
              enabled: true
            prometheusRule:
              enabled: true
              rules:
                - alert: NGINXConfigFailed
                  expr: count(nginx_ingress_controller_config_last_reload_successful == 0) > 0
                  for: 1s
                  labels:
                    severity: critical
                  annotations:
                    description: bad ingress config - nginx config test failed
                    summary: uninstall the latest ingress changes to allow config reloads to resume
                - alert: NGINXCertificateExpiry
                  expr: (avg(nginx_ingress_controller_ssl_expire_time_seconds) by (host) - time()) < 604800
                  for: 1s
                  labels:
                    severity: critical
                  annotations:
                    description: ssl certificate(s) will expire in less then a week
                    summary: renew expiring certificates to avoid downtime
                - alert: NGINXTooMany500s
                  expr: 100 * ( sum( nginx_ingress_controller_requests{status=~"5.+"} ) / sum(nginx_ingress_controller_requests) ) > 5
                  for: 1m
                  labels:
                    severity: warning
                  annotations:
                    description: Too many 5XXs
                    summary: More than 5% of all requests returned 5XX, this requires your attention
                - alert: NGINXTooMany400s
                  expr: 100 * ( sum( nginx_ingress_controller_requests{status=~"4.+"} ) / sum(nginx_ingress_controller_requests) ) > 5
                  for: 1m
                  labels:
                    severity: warning
                  annotations:
                    description: Too many 4XXs
                    summary: More than 5% of all requests returned 4XX, this requires your attention
          affinity:
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: app.kubernetes.io/name
                      operator: In
                      values:
                      - ingress-nginx
                    - key: app.kubernetes.io/instance
                      operator: In
                      values:
                      - ingress-nginx
                    - key: app.kubernetes.io/component
                      operator: In
                      values:
                      - controller
                  topologyKey: kubernetes.io/hostname
    repoURL: https://kubernetes.github.io/ingress-nginx
    targetRevision: 4.10.1
  syncPolicy:
    automated:
      prune: true
    syncOptions:
    - CreateNamespace=true
    - ApplyOutOfSyncOnly=true
---
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: kube-prometheus-stack
  annotations:
    argocd.argoproj.io/compare-options: ServerSideDiff=true
spec:
  destination:
    name: in-cluster
    namespace: monitoring
  project: k3s
  source:
    chart: kube-prometheus-stack
    helm:
      values: |-
        crds:
          enabled: false
        prometheus:
          prometheusSpec:
            serviceMonitorSelectorNilUsesHelmValues: false
            ruleSelectorNilUsesHelmValues: false
            externalUrl: "https://prometheus.mgmt.sholdee.net"
            storageSpec:
              volumeClaimTemplate:
                spec:
                  storageClassName: longhorn
                  accessModes: ["ReadWriteOnce"]
                  resources:
                    requests:
                      storage: 50Gi
        alertmanager:
          alertmanagerSpec:
            useExistingSecret: true
            configSecret: alertmanager-secret
            externalUrl: "https://alertmgr.mgmt.sholdee.net"
            storage:
              volumeClaimTemplate:
                spec:
                  storageClassName: longhorn
                  accessModes: ["ReadWriteOnce"]
                  resources:
                    requests:
                      storage: 10Gi
        grafana:
          persistence:
            enabled: true
            type: sts
            storageClassName: longhorn
            accessModes: ["ReadWriteOnce"]
            size: 20Gi
        kubelet:
          enabled: true
          serviceMonitor:
            metricRelabelings:
              # Remove duplicate labels provided by k3s
              - action: keep
                sourceLabels: ["__name__"]
                regex: (apiserver_audit|apiserver_client|apiserver_delegated|apiserver_envelope|apiserver_storage|apiserver_webhooks|authentication_token|cadvisor_version|container_blkio|container_cpu|container_fs|container_last|container_memory|container_network|container_oom|container_processes|container|csi_operations|disabled_metric|get_token|go|hidden_metric|kubelet_certificate|kubelet_cgroup|kubelet_container|kubelet_containers|kubelet_cpu|kubelet_device|kubelet_graceful|kubelet_http|kubelet_lifecycle|kubelet_managed|kubelet_node|kubelet_pleg|kubelet_pod|kubelet_run|kubelet_running|kubelet_runtime|kubelet_server|kubelet_started|kubelet_volume|kubernetes_build|kubernetes_feature|machine_cpu|machine_memory|machine_nvm|machine_scrape|node_namespace|plugin_manager|prober_probe|process_cpu|process_max|process_open|process_resident|process_start|process_virtual|registered_metric|rest_client|scrape_duration|scrape_samples|scrape_series|storage_operation|volume_manager|volume_operation|workqueue)_(.+)
              - action: replace
                sourceLabels: ["node"]
                targetLabel: instance
              # Drop high cardinality labels
              - action: labeldrop
                regex: (uid)
              - action: labeldrop
                regex: (id|name)
              - action: drop
                sourceLabels: ["__name__"]
                regex: (rest_client_request_duration_seconds_bucket|rest_client_request_duration_seconds_sum|rest_client_request_duration_seconds_count)
        kubeApiServer:
          enabled: true
          serviceMonitor:
            metricRelabelings:
              # Remove duplicate labels provided by k3s
              - action: keep
                sourceLabels: ["__name__"]
                regex: (aggregator_openapi|aggregator_unavailable|apiextensions_openapi|apiserver_admission|apiserver_audit|apiserver_cache|apiserver_cel|apiserver_client|apiserver_crd|apiserver_current|apiserver_envelope|apiserver_flowcontrol|apiserver_init|apiserver_kube|apiserver_longrunning|apiserver_request|apiserver_requested|apiserver_response|apiserver_selfrequest|apiserver_storage|apiserver_terminated|apiserver_tls|apiserver_watch|apiserver_webhooks|authenticated_user|authentication|disabled_metric|etcd_bookmark|etcd_lease|etcd_request|field_validation|get_token|go|grpc_client|hidden_metric|kube_apiserver|kubernetes_build|kubernetes_feature|node_authorizer|pod_security|process_cpu|process_max|process_open|process_resident|process_start|process_virtual|registered_metric|rest_client|scrape_duration|scrape_samples|scrape_series|serviceaccount_legacy|serviceaccount_stale|serviceaccount_valid|watch_cache|workqueue)_(.+)
              # Drop high cardinality labels
              - action: drop
                sourceLabels: ["__name__"]
                regex: (apiserver|etcd|rest_client)_request(|_sli|_slo)_duration_seconds_bucket
              - action: drop
                sourceLabels: ["__name__"]
                regex: (apiserver_response_sizes_bucket|apiserver_watch_events_sizes_bucket)
        kubeEtcd:
          enabled: true
          endpoints:
            - 192.168.99.10
            - 192.168.99.11
            - 192.168.99.12
        kubeControllerManager:
          enabled: false
        kubeScheduler:
          enabled: false
        kubeProxy:
          enabled: false
    repoURL: https://prometheus-community.github.io/helm-charts
    targetRevision: 60.2.0
  syncPolicy:
    automated:
      prune: true
    syncOptions:
      - CreateNamespace=true
      - ServerSideApply=true
---
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: kubernetes-dashboard
spec:
  destination:
    name: in-cluster
    namespace: kubernetes-dashboard
  ignoreDifferences:
  - group: '*'
    kind: ConfigMap
    name: kubernetes-dashboard-web-settings
    jsonPointers:
    - /data
  - group: '*'
    kind: Secret
    name: kubernetes-dashboard-csrf
    jsonPointers:
    - /data/private.key
  - group: apps
    kind: Deployment
    jsonPointers:
    - /spec/template/metadata/annotations/checksum~1config
  project: k3s
  source:
    chart: kubernetes-dashboard
    repoURL: https://kubernetes.github.io/dashboard/
    targetRevision: 7.5.0
  syncPolicy:
    automated:
      prune: true
    syncOptions:
    - CreateNamespace=true
    - ApplyOutOfSyncOnly=true
    - RespectIgnoreDifferences=true
---
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: longhorn
spec:
  destination:
    name: in-cluster
    namespace: longhorn-system
  project: k3s
  source:
    chart: longhorn
    helm:
      parameters:
      - name: defaultSettings.defaultDataLocality
        value: best-effort
      - name: defaultSettings.concurrentAutomaticEngineUpgradePerNodeLimit
        value: "2"
      - name: defaultSettings.nodeDownPodDeletionPolicy
        value: delete-both-statefulset-and-deployment-pod
      - name: defaultSettings.nodeDrainPolicy
        value: allow-if-replica-is-stopped
      - name: defaultSettings.replicaAutoBalance
        value: best-effort
      - name: defaultSettings.backupTarget
        value: "s3://sholdee-longhorn@us-west-002/"
      - name: defaultSettings.backupTargetCredentialSecret
        value: longhorn-s3-key
      - name: metrics.serviceMonitor.enabled
        value: "true"
    repoURL: https://charts.longhorn.io
    targetRevision: 1.6.2
  syncPolicy:
    automated:
      prune: true
    syncOptions:
    - ApplyOutOfSyncOnly=true
    - CreateNamespace=true
---
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: sealed-secrets-controller
spec:
  destination:
    name: in-cluster
    namespace: kube-system
  project: k3s
  source:
    chart: sealed-secrets
    repoURL: https://bitnami-labs.github.io/sealed-secrets
    targetRevision: 2.15.4
  syncPolicy:
    automated:
      prune: true
    syncOptions:
    - ApplyOutOfSyncOnly=true
---
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: traefik
spec:
  destination:
    name: in-cluster
    namespace: traefik
  project: k3s
  source:
    chart: traefik
    helm:
      values: |-
        service:
          spec:
            loadBalancerIP: 192.168.77.22
            externalTrafficPolicy: Cluster
        ports:
          web:
            redirectTo: 
                port: websecure
          websecure:
            tls:
              enabled: true
        additionalArguments:
          - --serversTransport.insecureSkipVerify=true
        providers:
          kubernetesIngress:
            publishedService:
              enabled: true
        deployment:
          replicas: 2
        podDisruptionBudget:
          enabled: true
          minAvailable: 1
        topologySpreadConstraints:
          - maxSkew: 1
            topologyKey: kubernetes.io/hostname
            whenUnsatisfiable: DoNotSchedule
            labelSelector:
              matchLabels:
                app.kubernetes.io/name: traefik
        logs:
          general:
            format: common
            level: INFO
          access:
            enabled: true
            format: common
        metrics:
          service:
            enabled: true
          disableAPICheck: false
          serviceMonitor:
            namespace: "traefik"
            metricRelabelings:
              - sourceLabels: [__name__]
                separator: ;
                regex: ^fluentd_output_status_buffer_(oldest|newest)_.+
                replacement: $1
                action: drop
            relabelings:
              - sourceLabels: [__meta_kubernetes_pod_node_name]
                separator: ;
                regex: ^(.*)$
                targetLabel: nodename
                replacement: $1
                action: replace
            jobLabel: traefik
            interval: 30s
            honorLabels: true
          prometheusRule:
            namespace: "traefik"
            rules:
              - alert: TraefikDown
                expr: up{job="traefik"} == 0
                for: 5m
                labels:
                  context: traefik
                  severity: critical
                annotations:
                  summary: "Traefik Down"
                  description: "{{ $labels.pod }} on {{ $labels.nodename }} is down"
    repoURL: https://helm.traefik.io/traefik
    targetRevision: 28.3.0
  syncPolicy:
    automated:
      prune: true
    syncOptions:
    - CreateNamespace=true
    - ApplyOutOfSyncOnly=true
---
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: velero
spec:
  destination:
    name: in-cluster
    namespace: velero
  project: k3s
  source:
    chart: velero
    helm:
      values: |-
        upgradeCRDs: true
        snapshotsEnabled: false
        credentials:
          existingSecret: cloud-credentials
        
        configuration:
          backupStorageLocation:
            - name: default
              provider: aws
              bucket: sholdee-velero
              default: true
              config:
                region: us-west-002
                s3ForcePathStyle: true
                s3Url: https://s3.us-west-002.backblazeb2.com
                checksumAlgorithm: ""
        
        initContainers:
          - name: velero-plugin-for-aws
            image: velero/velero-plugin-for-aws:v1.10.0
            imagePullPolicy: IfNotPresent
            volumeMounts:
              - mountPath: /target
                name: plugins
        schedules:
          k3s-backup-schedule:
            disabled: false
            schedule: "0 2 * * *"
            useOwnerReferencesInBackup: true
            template:
              storageLocation: default
              excludedResources:
                - persistentvolumes
                - persistentvolumeclaims
                - backuptargets.longhorn.io
                - backupvolumes.longhorn.io
                - backups.longhorn.io
                - nodes.longhorn.io
                - volumes.longhorn.io
                - engines.longhorn.io
                - replicas.longhorn.io
                - backingimagedatasources.longhorn.io
                - backingimagemanagers.longhorn.io
                - backingimages.longhorn.io
                - sharemanagers.longhorn.io
                - instancemanagers.longhorn.io
                - engineimages.longhorn.io
        metrics:
          serviceMonitor:
            enabled: true
          prometheusRule:
            enabled: true
            spec:
             - alert: VeleroBackupPartialFailures
               annotations:
                 message: Velero backup {{ $labels.schedule }} has {{ $value | humanizePercentage }} partialy failed backups.
               expr: |-
                 velero_backup_partial_failure_total{schedule!=""} / velero_backup_attempt_total{schedule!=""} > 0.25
               for: 15m
               labels:
                 severity: warning
             - alert: VeleroBackupFailures
               annotations:
                 message: Velero backup {{ $labels.schedule }} has {{ $value | humanizePercentage }} failed backups.
               expr: |-
                 velero_backup_failure_total{schedule!=""} / velero_backup_attempt_total{schedule!=""} > 0.25
               for: 15m
               labels:
                 severity: warning
    repoURL: https://vmware-tanzu.github.io/helm-charts
    targetRevision: 6.7.0
  syncPolicy:
    automated:
      prune: true
    syncOptions:
    - CreateNamespace=true
    - ApplyOutOfSyncOnly=true
---
