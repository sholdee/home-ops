apiVersion: argoproj.io/v1alpha1
kind: AppProject
metadata:
  name: k3s
spec:
  clusterResourceWhitelist:
  - group: '*'
    kind: '*'
  description: Home automation and management
  destinations:
  - name: in-cluster
    namespace: '*'
    server: https://kubernetes.default.svc
  namespaceResourceWhitelist:
  - group: '*'
    kind: '*'
  sourceRepos:
  - git@github.com:sholdee/k3s-app-manifests.git
---
apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: k3s-apps
spec:
  goTemplate: true
  goTemplateOptions: ["missingkey=error"]
  generators:
    - git:
        repoURL: 'git@github.com:sholdee/k3s-app-manifests.git'
        revision: master
        directories:
          - path: 'apps/*'
  template:
    metadata:
      name: '{{.path.basename}}'
    spec:
      project: k3s
      source:
        repoURL: 'git@github.com:sholdee/k3s-app-manifests.git'
        targetRevision: master
        path: '{{.path.path}}'
        directory:
          recurse: true
      destination:
        name: in-cluster
        namespace: '{{ regexReplaceAll "-conf$" .path.basename "" }}'
      syncPolicy:
        automated:
          prune: true
        syncOptions:
        - CreateNamespace=true
        - ApplyOutOfSyncOnly=true
---
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: argocd
spec:
  destination:
    name: in-cluster
    namespace: argocd
  project: k3s
  source:
    chart: argo-cd
    helm:
      values: |-
        configs:
          cm:
            resource.exclusions: |
             - apiGroups:
                 - cilium.io
               kinds:
                 - CiliumIdentity
               clusters:
                 - "*"
    repoURL: https://argoproj.github.io/argo-helm
    targetRevision: 7.1.3
  syncPolicy:
    automated:
      prune: true
    syncOptions:
    - CreateNamespace=true
    - ApplyOutOfSyncOnly=true
---
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: cert-manager
spec:
  destination:
    name: in-cluster
    namespace: cert-manager
  project: k3s
  source:
    chart: cert-manager
    helm:
      parameters:
      - name: installCRDs
        value: "true"
      - name: dns01RecursiveNameservers
        value: 1.1.1.1:53,1.0.0.1:53
      - name: dns01RecursiveNameserversOnly
        value: "true"
    repoURL: https://charts.jetstack.io
    targetRevision: v1.15.0
  syncPolicy:
    automated:
      prune: true
    syncOptions:
    - CreateNamespace=true
    - ApplyOutOfSyncOnly=true
---
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: cilium
spec:
  destination:
    name: in-cluster
    namespace: kube-system
  ignoreDifferences:
    - jsonPointers:
      - /data/ca.crt
      - /data/ca.key
      kind: Secret
      name: cilium-ca
    - jsonPointers:
      - /data/ca.crt
      kind: Secret
      name: hubble-ca-cert
    - jsonPointers:
      - /data/ca.crt
      - /data/tls.crt
      - /data/tls.key
      kind: Secret
      name: hubble-relay-client-certs
    - jsonPointers:
      - /data/ca.crt
      - /data/tls.crt
      - /data/tls.key
      kind: Secret
      name: hubble-server-certs
  project: k3s
  source:
    chart: cilium
    helm:
      values: |-
        operator:
          replicas: "1"
        ipam:
          operator:
            clusterPoolIPv4PodCIDRList: "10.52.0.0/16"
        ipv4NativeRoutingCIDR: "10.52.0.0/16"
        k8sServiceHost: "127.0.0.1"
        k8sServicePort: "6444"
        routingMode: "native"
        autoDirectNodeRoutes: true
        kubeProxyReplacement: true
        bpf:
          masquerade: false
          loadBalancer:
            algorithm: "maglev"
            mode: "hybrid"
        enableIPv4Masquerade: false
        bgpControlPlane:
          enabled: true
        hubble:
          enabled: true
          relay:
            enabled: true
          ui:
            enabled: true
        installNoConntrackIptablesRules: true
    repoURL: https://helm.cilium.io/
    targetRevision: 1.15.6
  syncPolicy:
    automated:
      prune: true
    syncOptions:
    - ApplyOutOfSyncOnly=true
    - RespectIgnoreDifferences=true
---
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: ingress-nginx
spec:
  destination:
    name: in-cluster
    namespace: ingress-nginx
  project: k3s
  source:
    chart: ingress-nginx
    helm:
      parameters:
      - name: controller.service.internal.loadBalancerIP
        value: 192.168.77.20
      - name: controller.metrics.enabled
        value: "true"
      - name: controller.service.externalTrafficPolicy
        value: Cluster
      - name: controller.replicaCount
        value: "1"
      values: |-
        controller.podAnnotations."prometheus\.io/scrape": 'true'
        controller.podAnnotations."prometheus\.io/port": '10254'
        controller:
          affinity:
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: app.kubernetes.io/name
                      operator: In
                      values:
                      - ingress-nginx
                    - key: app.kubernetes.io/instance
                      operator: In
                      values:
                      - ingress-nginx
                    - key: app.kubernetes.io/component
                      operator: In
                      values:
                      - controller
                  topologyKey: kubernetes.io/hostname
    repoURL: https://kubernetes.github.io/ingress-nginx
    targetRevision: 4.10.1
  syncPolicy:
    automated:
      prune: true
    syncOptions:
    - CreateNamespace=true
    - ApplyOutOfSyncOnly=true
---
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: kube-prometheus-stack
spec:
  destination:
    name: in-cluster
    namespace: monitoring
  project: k3s
  source:
    chart: kube-prometheus-stack
    helm:
      values: |-
        prometheus:
          prometheusSpec:
            retention: 10d
            storageSpec:
              volumeClaimTemplate:
                spec:
                  storageClassName: longhorn
                  accessModes: ["ReadWriteOnce"]
                  resources:
                    requests:
                      storage: 50Gi
        alertmanager:
          alertmanagerSpec:
            storage:
              volumeClaimTemplate:
                spec:
                  storageClassName: longhorn
                  accessModes: ["ReadWriteOnce"]
                  resources:
                    requests:
                      storage: 10Gi
        grafana:
          adminPassword: "admin"
          persistence:
            enabled: true
            storageClassName: longhorn
            accessModes: ["ReadWriteOnce"]
            size: 10Gi
          plugins:
            - grafana-piechart-panel
            - grafana-simple-json-datasource
        prometheusOperator:
          createCustomResource: true
        kubeStateMetrics:
          enabled: true
        nodeExporter:
          enabled: true
        alertmanager:
          enabled: true
        prometheus:
          enabled: true
        grafana:
          enabled: true
        additionalPrometheusRules:
          - name: "longhorn.rules"
            groups:
              - name: "longhorn.rules"
                rules:
                  - alert: LonghornVolumeUsageHigh
                    expr: (longhorn_volume_usage_percentage > 85)
                    for: 2m
                    labels:
                      severity: warning
                    annotations:
                      summary: "Longhorn volume usage is over 85% ({{ $labels.volume }})"
                      description: "Volume usage is above 85%\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - name: "kube.rules"
            groups:
              - name: "kube.rules"
                rules:
                  - alert: NodeDiskRunningFull
                    expr: (node_filesystem_free_bytes{mountpoint=\"/\"} / node_filesystem_size_bytes{mountpoint=\"/\"}) < 0.15
                    for: 10m
                    labels:
                      severity: warning
                    annotations:
                      summary: "Node root filesystem is running full"
                      description: "Node root filesystem is running full (< 15% left)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
        rbac:
          create: true
        serviceMonitor:
          selfMonitor: true
          additionalEndpoints: []
    repoURL: https://prometheus-community.github.io/helm-charts
    targetRevision: 60.0.2
  syncPolicy:
    automated:
      prune: true
    syncOptions:
      - CreateNamespace=true
      - ApplyOutOfSyncOnly=true
---
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: kubernetes-dashboard
spec:
  destination:
    name: in-cluster
    namespace: kubernetes-dashboard
  ignoreDifferences:
  - group: '*'
    kind: ConfigMap
    name: kubernetes-dashboard-web-settings
    jsonPointers:
    - /data
  - group: '*'
    kind: Secret
    name: kubernetes-dashboard-csrf
    jsonPointers:
    - /data/private.key
  - group: apps
    kind: Deployment
    jsonPointers:
    - /spec/template/metadata/annotations/checksum~1config
  project: k3s
  source:
    chart: kubernetes-dashboard
    repoURL: https://kubernetes.github.io/dashboard/
    targetRevision: 7.5.0
  syncPolicy:
    automated:
      prune: true
    syncOptions:
    - CreateNamespace=true
    - ApplyOutOfSyncOnly=true
    - RespectIgnoreDifferences=true
---
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: longhorn
spec:
  destination:
    name: in-cluster
    namespace: longhorn-system
  project: k3s
  source:
    chart: longhorn
    helm:
      parameters:
      - name: defaultSettings.defaultDataLocality
        value: best-effort
      - name: defaultSettings.concurrentAutomaticEngineUpgradePerNodeLimit
        value: "2"
      - name: defaultSettings.nodeDownPodDeletionPolicy
        value: delete-both-statefulset-and-deployment-pod
      - name: defaultSettings.nodeDrainPolicy
        value: allow-if-replica-is-stopped
      - name: defaultSettings.replicaAutoBalance
        value: best-effort
      - name: defaultSettings.backupTarget
        value: "s3://sholdee-longhorn@us-west-002/"
      - name: defaultSettings.backupTargetCredentialSecret
        value: longhorn-s3-key
    repoURL: https://charts.longhorn.io
    targetRevision: 1.6.2
  syncPolicy:
    automated:
      prune: true
    syncOptions:
    - ApplyOutOfSyncOnly=true
    - CreateNamespace=true
---
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: sealed-secrets-controller
spec:
  destination:
    name: in-cluster
    namespace: kube-system
  project: k3s
  source:
    chart: sealed-secrets
    repoURL: https://bitnami-labs.github.io/sealed-secrets
    targetRevision: 2.15.4
  syncPolicy:
    automated:
      prune: true
    syncOptions:
    - ApplyOutOfSyncOnly=true
---
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: traefik
spec:
  destination:
    name: in-cluster
    namespace: traefik
  project: k3s
  source:
    chart: traefik
    helm:
      values: |-
        service:
          spec:
            loadBalancerIP: 192.168.77.22
            externalTrafficPolicy: Cluster
        ports:
          web:
            redirectTo: 
                port: websecure
          websecure:
            tls:
              enabled: true
        additionalArguments:
          - --serversTransport.insecureSkipVerify=true
        providers:
          kubernetesIngress:
            publishedService:
              enabled: true
    repoURL: https://helm.traefik.io/traefik
    targetRevision: 28.2.0
  syncPolicy:
    automated:
      prune: true
    syncOptions:
    - CreateNamespace=true
    - ApplyOutOfSyncOnly=true
---
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: velero
spec:
  destination:
    name: in-cluster
    namespace: velero
  project: k3s
  source:
    chart: velero
    helm:
      values: |-
        snapshotsEnabled: false
        credentials:
          existingSecret: cloud-credentials
        
        configuration:
          backupStorageLocation:
            - name: default
              provider: aws
              bucket: sholdee-velero
              default: true
              config:
                region: us-west-002
                s3ForcePathStyle: true
                s3Url: https://s3.us-west-002.backblazeb2.com
                checksumAlgorithm: ""
        
        initContainers:
          - name: velero-plugin-for-aws
            image: velero/velero-plugin-for-aws:v1.9.2
            imagePullPolicy: IfNotPresent
            volumeMounts:
              - mountPath: /target
                name: plugins
        schedules:
          k3s-backup-schedule:
            disabled: false
            schedule: "0 2 * * *"
            useOwnerReferencesInBackup: true
            template:
              storageLocation: default
              excludedResources:
                - persistentvolumes
                - persistentvolumeclaims
                - backuptargets.longhorn.io
                - backupvolumes.longhorn.io
                - backups.longhorn.io
                - nodes.longhorn.io
                - volumes.longhorn.io
                - engines.longhorn.io
                - replicas.longhorn.io
                - backingimagedatasources.longhorn.io
                - backingimagemanagers.longhorn.io
                - backingimages.longhorn.io
                - sharemanagers.longhorn.io
                - instancemanagers.longhorn.io
                - engineimages.longhorn.io
    repoURL: https://vmware-tanzu.github.io/helm-charts
    targetRevision: 6.6.0
  syncPolicy:
    automated:
      prune: true
    syncOptions:
    - CreateNamespace=true
    - ApplyOutOfSyncOnly=true
---
